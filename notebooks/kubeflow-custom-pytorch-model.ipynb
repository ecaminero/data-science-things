{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33abccfd",
   "metadata": {},
   "source": [
    "# kubeflow pipeline with Pythorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install components\n",
    "\n",
    "!pip3 install -q google-cloud-aiplatform\n",
    "!pip3 install -q  kfp google-cloud-pipeline-components"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "214831e5",
   "metadata": {},
   "source": [
    "Enable this apis\n",
    "* Cloud Build API\n",
    "* Artifact Registry\n",
    "* Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade32a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After installing these packages you'll need to restart the kernel\n",
    "import os\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d53d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, check that you have correctly installed the packages. The KFP SDK version\n",
    "!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "!python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9381f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e078eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Cloud project ID cand Region\n",
    "PROJECT_ID = \"[your-project-id]\"\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "\n",
    "# Setting Region\n",
    "PROJECT_REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "if PROJECT_REGION == \"[your-region]\":\n",
    "    PROJECT_REGION = \"us-central1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a82b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Bucket \n",
    "BUCKET_URI = f\"gs://aip-{PROJECT_ID}\"\n",
    "GENERATE_BUCKET_URI = False  # @param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "if GENERATE_BUCKET_URI:\n",
    "    bucket_name = \"gs://aip-{}\".format(PROJECT_ID)\n",
    "    !gsutil mb -p {PROJECT_ID} -l {REGION} {bucket_name}\n",
    "\n",
    "    # set GCS bucket object TTL to 7 days\n",
    "    !echo '{\"rule\":[{\"action\": {\"type\": \"Delete\"},\"condition\": {\"age\": 7}}]}' > gcs_lifecycle.tmp\n",
    "    !gsutil lifecycle set gcs_lifecycle.tmp {bucket_name}\n",
    "    !rm gcs_lifecycle.tmp\n",
    "\n",
    "    BUCKET_URI = bucket_name\n",
    "    print(f\"changed BUCKET_URI to {BUCKET_URI} due to GENERATE_BUCKET_URI is True\")\n",
    "\n",
    "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_URI = f\"gs://aip-{PROJECT_ID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d39035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup up the following constants for Vertex AI Pipelines:\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/minst\".format(BUCKET_URI)\n",
    "PACKAGE_PATH = \"tmp/image classification pipeline.json\".replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae85a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline directory locally to save the component and pipeline specifications\n",
    "!mkdir -p ./tmp/pipelines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef3eaf24",
   "metadata": {},
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdcaa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import NamedTuple\n",
    "\n",
    "import google_cloud_pipeline_components\n",
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "from google_cloud_pipeline_components import aiplatform as aip_components\n",
    "from google_cloud_pipeline_components.experimental import custom_job\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import Input, Metrics, Model, Output, component"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c8fd40d",
   "metadata": {},
   "source": [
    "### Creating Custom model\n",
    "consider a pipeline with the following steps:\n",
    "\n",
    "* Ingest data: This step loads training data into the pipeline.\n",
    "* Preprocess data: This step preprocesses the ingested training data.\n",
    "* Train model: This step uses the preprocessed training data to train a model.\n",
    "* Evaluate model: This step evaluates the trained model.\n",
    "* Deploy: This step deploys the trained model for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usefull function to get the latest run of a pipeline\n",
    "from datetime import datetime\n",
    "def get_timestamp():\n",
    "    return datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "TIMESTAMP = get_timestamp()\n",
    "print(f\"TIMESTAMP = {TIMESTAMP}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47c00893",
   "metadata": {},
   "source": [
    "### Copy training application code and Dockerfile from local path to GCS location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b27016",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "APP_NAME = \"finetuned-bert-classifier\"\n",
    "# copy training Dockerfile\n",
    "!gsutil -m cp ../models/finetuned-bert-classifier/container/Dockerfile {BUCKET_URI}/{APP_NAME}/train/\n",
    "# copy training application code\n",
    "!gsutil -m cp -r ../models/finetuned-bert-classifier/python_package/trainer/ {BUCKET_URI}/{APP_NAME}/train/\n",
    "print(f\"Copied training application code and Dockerfile to {BUCKET_URI}/{APP_NAME}/train/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "481a78c8",
   "metadata": {},
   "source": [
    "#### Define custom pipeline component to build custom training container"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b43f316f",
   "metadata": {},
   "source": [
    "There are a few things to notice about the component specification:\n",
    "\n",
    "* The standalone function defined is converted as a pipeline component using the @kfp.v2.dsl.component decorator.\n",
    "* All the arguments in the standalone function must have data type annotations because KFP uses the function’s inputs and outputs to define the component’s interface.\n",
    "* By default Python 3.7 is used as the base image to run the code defined. You can configure the @component decorator to override the default image by specifying base_image, install additional python packages using packages_to_install parameter and write the compiled component file as a YAML file using output_component_file to share or reuse the component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc369299",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/google.com/cloudsdktool/cloud-sdk:latest\",\n",
    "    packages_to_install=[\"google-cloud-build\"],\n",
    "    output_component_file=\"./tmp/pipelines/build_custom_train_image.yaml\",\n",
    ")\n",
    "def build_train_image(\n",
    "    project: str, gs_train_src_path: str, training_image_uri: str\n",
    ") -> NamedTuple(\"Outputs\", [(\"training_image_uri\", str)]):\n",
    "    \"\"\"custom pipeline component to build custom training image using\n",
    "    Cloud Build and the training application code and dependencies\n",
    "    defined in the Dockerfile\n",
    "    \"\"\"\n",
    "\n",
    "    import logging\n",
    "    import os\n",
    "\n",
    "    from google.cloud.devtools import cloudbuild_v1 as cloudbuild\n",
    "    from google.protobuf.duration_pb2 import Duration\n",
    "\n",
    "    # initialize client for cloud build\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    client = cloudbuild.services.cloud_build.CloudBuildClient()\n",
    "\n",
    "    # parse step inputs to get path to Dockerfile and training application code\n",
    "    gs_dockerfile_path = os.path.join(gs_train_src_path, \"Dockerfile\")\n",
    "    gs_train_src_path = os.path.join(gs_train_src_path, \"trainer/\")\n",
    "\n",
    "    logging.info(f\"training_image_uri: {training_image_uri}\")\n",
    "\n",
    "    # define build steps to pull the training code and Dockerfile\n",
    "    # and build/push the custom training container image\n",
    "    build = cloudbuild.Build()\n",
    "    build.steps = [\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "            \"args\": [\"cp\", \"-r\", gs_train_src_path, \".\"],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "            \"args\": [\"cp\", gs_dockerfile_path, \"Dockerfile\"],\n",
    "        },\n",
    "        # enabling Kaniko cache in a Docker build that caches intermediate\n",
    "        # layers and pushes image automatically to Container Registry\n",
    "        # https://cloud.google.com/build/docs/kaniko-cache\n",
    "        {\n",
    "            \"name\": \"gcr.io/kaniko-project/executor:latest\",\n",
    "            \"args\": [f\"--destination={training_image_uri}\", \"--cache=true\"],\n",
    "        },\n",
    "    ]\n",
    "    # override default timeout of 10min\n",
    "    timeout = Duration()\n",
    "    timeout.seconds = 7200\n",
    "    build.timeout = timeout\n",
    "\n",
    "    # create build\n",
    "    operation = client.create_build(project_id=project, build=build)\n",
    "    logging.info(\"IN PROGRESS:\")\n",
    "    logging.info(operation.metadata)\n",
    "\n",
    "    # get build status\n",
    "    result = operation.result()\n",
    "    logging.info(\"RESULT:\", result.status)\n",
    "\n",
    "    # return step outputs\n",
    "    return (training_image_uri,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd4954db",
   "metadata": {},
   "source": [
    "## Component: Get Custom Training Job Details from Vertex AI\n",
    "This step gets details from a custom training job from Vertex AI including training elapsed time, model performance metrics that will be used in the next step before the model deployment. The step additionally creates Model artifact with trained model artifacts.\n",
    "\n",
    "NOTE: The pre-built custom job component used in the pipeline outputs CustomJob resource but not the model artifacts.\n",
    "\n",
    "* Inputs:\n",
    "\n",
    "    * job_resource: Custom job resource returned by pre-built CustomJob component\n",
    "    * project: Project ID where the job ran\n",
    "    * region: Region where the job ran\n",
    "    * eval_metric_key: Evaluation metric key name such as eval_accuracy\n",
    "    *  model_display_name: Model display name for saving model artifacts\n",
    "Outputs:\n",
    "\n",
    "* model: Trained model artifacts created by the training job with added model metadata\n",
    "* metrics: Model performance metrics captured from the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-pipeline-components\",\n",
    "        \"google-cloud-aiplatform\",\n",
    "        \"pandas\",\n",
    "        \"fsspec\",\n",
    "    ],\n",
    "    output_component_file=\"./tmp/pipelines/get_training_job_details.yaml\",\n",
    ")\n",
    "def get_training_job_details(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    job_resource: str,\n",
    "    eval_metric_key: str,\n",
    "    model_display_name: str,\n",
    "    metrics: Output[Metrics],\n",
    "    model: Output[Model],\n",
    ") -> NamedTuple(\n",
    "    \"Outputs\", [(\"eval_metric\", float), (\"eval_loss\", float), (\"model_artifacts\", str)]\n",
    "):\n",
    "    \"\"\"custom pipeline component to get model artifacts and performance\n",
    "    metrics from custom training job\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    import shutil\n",
    "    from collections import namedtuple\n",
    "\n",
    "    import pandas as pd\n",
    "    from google.cloud.aiplatform import gapic as aip\n",
    "    from google.protobuf.json_format import Parse\n",
    "    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import \\\n",
    "        GcpResources\n",
    "\n",
    "    # parse training job resource\n",
    "    logging.info(f\"Custom job resource = {job_resource}\")\n",
    "    training_gcp_resources = Parse(job_resource, GcpResources())\n",
    "    custom_job_id = training_gcp_resources.resources[0].resource_uri\n",
    "    custom_job_name = \"/\".join(custom_job_id.split(\"/\")[-6:])\n",
    "    logging.info(f\"Custom job name parsed = {custom_job_name}\")\n",
    "\n",
    "    # get custom job information\n",
    "    API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(location)\n",
    "    client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "    job_client = aip.JobServiceClient(client_options=client_options)\n",
    "    job_resource = job_client.get_custom_job(name=custom_job_name)\n",
    "    job_base_dir = job_resource.job_spec.base_output_directory.output_uri_prefix\n",
    "    logging.info(f\"Custom job base output directory = {job_base_dir}\")\n",
    "\n",
    "    # copy model artifacts\n",
    "    logging.info(f\"Copying model artifacts to {model.path}\")\n",
    "    destination = shutil.copytree(job_base_dir.replace(\"gs://\", \"/gcs/\"), model.path)\n",
    "    logging.info(destination)\n",
    "    logging.info(f\"Model artifacts located at {model.uri}/model/{model_display_name}\")\n",
    "    logging.info(f\"Model artifacts located at model.uri = {model.uri}\")\n",
    "\n",
    "    # set model metadata\n",
    "    start, end = job_resource.start_time, job_resource.end_time\n",
    "    model.metadata[\"model_name\"] = model_display_name\n",
    "    model.metadata[\"framework\"] = \"pytorch\"\n",
    "    model.metadata[\"job_name\"] = custom_job_name\n",
    "    model.metadata[\"time_to_train_in_seconds\"] = (end - start).total_seconds()\n",
    "\n",
    "    # fetch metrics from the training job run\n",
    "    metrics_uri = f\"{model.path}/model/{model_display_name}/all_results.json\"\n",
    "    logging.info(f\"Reading and logging metrics from {metrics_uri}\")\n",
    "    metrics_df = pd.read_json(metrics_uri, typ=\"series\")\n",
    "    for k, v in metrics_df.items():\n",
    "        logging.info(f\"     {k} -> {v}\")\n",
    "        metrics.log_metric(k, v)\n",
    "\n",
    "    # capture eval metric and log to model metadata\n",
    "    eval_metric = (\n",
    "        metrics_df[eval_metric_key] if eval_metric_key in metrics_df.keys() else None\n",
    "    )\n",
    "    eval_loss = metrics_df[\"eval_loss\"] if \"eval_loss\" in metrics_df.keys() else None\n",
    "    logging.info(f\"     {eval_metric_key} -> {eval_metric}\")\n",
    "    logging.info(f'     \"eval_loss\" -> {eval_loss}')\n",
    "\n",
    "    model.metadata[eval_metric_key] = eval_metric\n",
    "    model.metadata[\"eval_loss\"] = eval_loss\n",
    "\n",
    "    # return output parameters\n",
    "    outputs = namedtuple(\"Outputs\", [\"eval_metric\", \"eval_loss\", \"model_artifacts\"])\n",
    "\n",
    "    return outputs(eval_metric, eval_loss, job_base_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b42c5683",
   "metadata": {},
   "source": [
    "### Component: Create Model Archive (MAR) file using Torch Model Archiver\n",
    "This step packages trained model artifacts and custom prediction handler  as a model archive (.mar) file usign Torch Model Archiver tool.\n",
    "Example: https://github.com/pytorch/serve/tree/master/model-archiver\n",
    "* Inputs:\n",
    "\n",
    "    * model_display_name: Model display name for saving model archive file\n",
    "    * model_version: Model version for saving model archive file\n",
    "    * handler: Location of custom prediction handler\n",
    "    * model: Trained model artifacts from the previous step\n",
    "\n",
    "* Outputs:\n",
    "\n",
    "    * model_mar: Packaged model archive file (artifact) on GCS\n",
    "    * mar_env: A list of environment variables required for creating model resource\n",
    "    * mar_export_uri: GCS path to the model archive file\n",
    "\n",
    "Copy custom prediction handler code from local path to GCS location"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db6e6803",
   "metadata": {},
   "source": [
    "Define custom pipeline component to create model archive file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30aab7b0",
   "metadata": {},
   "source": [
    "Define custom pipeline component to build custom serving container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f838044",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"torch-model-archiver\"], \n",
    "    output_component_file=\"./tmp/pipelines/generate_mar_file.yaml\",\n",
    ")\n",
    "def generate_mar_file(\n",
    "    model_display_name: str,\n",
    "    model_version: str,\n",
    "    handler: str,\n",
    "    model: Input[Model],\n",
    "    model_mar: Output[Model],\n",
    ") -> NamedTuple(\"Outputs\", [(\"mar_env_var\", list), (\"mar_export_uri\", str)]):\n",
    "    \"\"\"custom pipeline component to package model artifacts and custom\n",
    "    handler to a model archive file using Torch Model Archiver tool\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    import os\n",
    "    import subprocess\n",
    "    import time\n",
    "    from collections import namedtuple\n",
    "    from pathlib import Path\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    # create directory to save model archive file\n",
    "    model_output_root = model.path\n",
    "    mar_output_root = model_mar.path\n",
    "    export_path = f\"{mar_output_root}/model-store\"\n",
    "    try:\n",
    "        Path(export_path).mkdir(parents=True, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        logging.warning(e)\n",
    "        # retry after pause\n",
    "        time.sleep(2)\n",
    "        Path(export_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # parse and configure paths for model archive config\n",
    "    handler_path = (\n",
    "        handler.replace(\"gs://\", \"/gcs/\") + \"predictor/handler.py\"\n",
    "        if handler.startswith(\"gs://\")\n",
    "        else handler\n",
    "    )\n",
    "    model_artifacts_dir = f\"{model_output_root}/model/{model_display_name}\"\n",
    "    extra_files = [\n",
    "        os.path.join(model_artifacts_dir, f)\n",
    "        for f in os.listdir(model_artifacts_dir)\n",
    "        if f != \"pytorch_model.bin\"\n",
    "    ]\n",
    "\n",
    "    # define model archive config\n",
    "    mar_config = {\n",
    "        \"MODEL_NAME\": model_display_name,\n",
    "        \"HANDLER\": handler_path,\n",
    "        \"SERIALIZED_FILE\": f\"{model_artifacts_dir}/pytorch_model.bin\",\n",
    "        \"VERSION\": model_version,\n",
    "        \"EXTRA_FILES\": \",\".join(extra_files),\n",
    "        \"EXPORT_PATH\": f\"{model_mar.path}/model-store\",\n",
    "    }\n",
    "\n",
    "    # generate model archive command\n",
    "    archiver_cmd = (\n",
    "        \"torch-model-archiver --force \"\n",
    "        f\"--model-name {mar_config['MODEL_NAME']} \"\n",
    "        f\"--serialized-file {mar_config['SERIALIZED_FILE']} \"\n",
    "        f\"--handler {mar_config['HANDLER']} \"\n",
    "        f\"--version {mar_config['VERSION']}\"\n",
    "    )\n",
    "    if \"EXPORT_PATH\" in mar_config:\n",
    "        archiver_cmd += f\" --export-path {mar_config['EXPORT_PATH']}\"\n",
    "    if \"EXTRA_FILES\" in mar_config:\n",
    "        archiver_cmd += f\" --extra-files {mar_config['EXTRA_FILES']}\"\n",
    "    if \"REQUIREMENTS_FILE\" in mar_config:\n",
    "        archiver_cmd += f\" --requirements-file {mar_config['REQUIREMENTS_FILE']}\"\n",
    "\n",
    "    # run archiver command\n",
    "    logging.warning(\"Running archiver command: %s\", archiver_cmd)\n",
    "    with subprocess.Popen(\n",
    "        archiver_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
    "    ) as p:\n",
    "        _, err = p.communicate()\n",
    "        if err:\n",
    "            raise ValueError(err)\n",
    "\n",
    "    # set output variables\n",
    "    mar_env_var = [{\"name\": \"MODEL_NAME\", \"value\": model_display_name}]\n",
    "    mar_export_uri = f\"{model_mar.uri}/model-store/\"\n",
    "\n",
    "    outputs = namedtuple(\"Outputs\", [\"mar_env_var\", \"mar_export_uri\"])\n",
    "    return outputs(mar_env_var, mar_export_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b9d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy custom prediction handler\n",
    "!gsutil cp ../models/finetuned-bert-classifier/predictor/handler.py {BUCKET_URI}/{APP_NAME}/serve/predictor/handler.py\n",
    "!gsutil cp ../models/finetuned-bert-classifier/predictor/Dockerfile.serve {BUCKET_URI}/{APP_NAME}/serve/predictor/Dockerfile.serve\n",
    "!gsutil cp ../models/finetuned-bert-classifier/predictor/index_to_name.json {BUCKET_URI}/{APP_NAME}/serve/predictor/index_to_name.json\n",
    "\n",
    "# list copied files from GCS location\n",
    "!gsutil ls -lR {BUCKET_URI}/{APP_NAME}/serve/\n",
    "\n",
    "print(f\"Copied custom prediction handler code to {BUCKET_URI}/{APP_NAME}/serve/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e27978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"google-cloud-build\"],\n",
    "    output_component_file=\"./tmp/pipelines/build_custom_serving_image.yaml\",\n",
    ")\n",
    "def build_custom_serving_image(\n",
    "    project: str, gs_serving_dependencies_path: str, serving_image_uri: str\n",
    ") -> NamedTuple(\"Outputs\", [(\"serving_image_uri\", str)],):\n",
    "    \"\"\"custom pipeline component to build custom serving image using\n",
    "    Cloud Build and dependencies defined in the Dockerfile\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    import os\n",
    "\n",
    "    from google.cloud.devtools import cloudbuild_v1 as cloudbuild\n",
    "    from google.protobuf.duration_pb2 import Duration\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    build_client = cloudbuild.services.cloud_build.CloudBuildClient()\n",
    "\n",
    "    logging.info(f\"gs_serving_dependencies_path: {gs_serving_dependencies_path}\")\n",
    "    gs_dockerfile_path = os.path.join(gs_serving_dependencies_path, \"Dockerfile.serve\")\n",
    "\n",
    "    logging.info(f\"serving_image_uri: {serving_image_uri}\")\n",
    "    build = cloudbuild.Build()\n",
    "    build.steps = [\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "            \"args\": [\"cp\", gs_dockerfile_path, \"Dockerfile\"],\n",
    "        },\n",
    "        # enabling Kaniko cache in a Docker build that caches intermediate\n",
    "        # layers and pushes image automatically to Container Registry\n",
    "        # https://cloud.google.com/build/docs/kaniko-cache\n",
    "        {\n",
    "            \"name\": \"gcr.io/kaniko-project/executor:latest\",\n",
    "            \"args\": [f\"--destination={serving_image_uri}\", \"--cache=true\"],\n",
    "        },\n",
    "    ]\n",
    "    # override default timeout of 10min\n",
    "    timeout = Duration()\n",
    "    timeout.seconds = 7200\n",
    "    build.timeout = timeout\n",
    "\n",
    "    # create build\n",
    "    operation = build_client.create_build(project_id=project, build=build)\n",
    "    logging.info(\"IN PROGRESS:\")\n",
    "    logging.info(operation.metadata)\n",
    "\n",
    "    # get build status\n",
    "    result = operation.result()\n",
    "    logging.info(\"RESULT:\", result.status)\n",
    "\n",
    "    # return step outputs\n",
    "    return (serving_image_uri,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92530517",
   "metadata": {},
   "source": [
    "#### Component: Create custom serving container running TorchServe\n",
    "\n",
    "The step builds a custom serving container running TorchServe HTTP server to serve prediction requests for the models mounted. The output from this step is the Container registry URI to the custom serving container.\n",
    "\n",
    "* Inputs:\n",
    "    * project: Project ID to run\n",
    "    * serving_image_uri: Custom serving container URI from Container registry\n",
    "    * gs_serving_dependencies_path: Location of serving dependencies - Dockerfile\n",
    "* Outputs:\n",
    "    * serving_image_uri: Custom serving container URI from Container registry\n",
    "    * Create Dockerfile from TorchServe CPU image as base, install required dependencies and run TorchServe serve command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec81746",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-aiplatform\", \n",
    "        \"google-cloud-pipeline-components\"\n",
    "    ],\n",
    "    output_component_file=\"./tmp/pipelines/make_prediction_request.yaml\",\n",
    ")\n",
    "def make_prediction_request(project: str, bucket: str, endpoint: str, instances: list):\n",
    "    \"\"\"custom pipeline component to pass prediction requests to Vertex AI\n",
    "    endpoint and get responses\n",
    "    \"\"\"\n",
    "    import base64\n",
    "    import logging\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "    from google.protobuf.json_format import Parse\n",
    "    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import \\\n",
    "        GcpResources\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    aiplatform.init(project=project, staging_bucket=bucket)\n",
    "\n",
    "    # parse endpoint resource\n",
    "    logging.info(f\"Endpoint = {endpoint}\")\n",
    "    gcp_resources = Parse(endpoint, GcpResources())\n",
    "    endpoint_uri = gcp_resources.resources[0].resource_uri\n",
    "    endpoint_id = \"/\".join(endpoint_uri.split(\"/\")[-8:-2])\n",
    "    logging.info(f\"Endpoint ID = {endpoint_id}\")\n",
    "\n",
    "    # define endpoint client\n",
    "    _endpoint = aiplatform.Endpoint(endpoint_id)\n",
    "\n",
    "    # call prediction endpoint for each instance\n",
    "    for instance in instances:\n",
    "        if not isinstance(instance, (bytes, bytearray)):\n",
    "            instance = instance.encode()\n",
    "        logging.info(f\"Input text: {instance.decode('utf-8')}\")\n",
    "        b64_encoded = base64.b64encode(instance)\n",
    "        test_instance = [{\"data\": {\"b64\": f\"{str(b64_encoded.decode('utf-8'))}\"}}]\n",
    "        response = _endpoint.predict(instances=test_instance)\n",
    "        logging.info(f\"Prediction response: {response.predictions}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfd7472e",
   "metadata": {},
   "source": [
    "## Define Pipeline Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "REGION = os.getenv(\"REGION\", \"us-central1\")\n",
    "APP_NAME = os.getenv(\"APP_NAME\", \"finetuned-bert-classifier\")\n",
    "VERSION = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "MODEL_NAME = APP_NAME\n",
    "BUCKET = BUCKET_URI\n",
    "MODEL_DISPLAY_NAME = f\"{MODEL_NAME}-{VERSION}\"\n",
    "\n",
    "PIPELINE_NAME = f\"pytorch-{APP_NAME}\"\n",
    "PIPELINE_ROOT = f\"{BUCKET}/pipeline_root/{MODEL_NAME}\"\n",
    "GCS_STAGING = f\"{BUCKET}/pipeline_root/{MODEL_NAME}\"\n",
    "\n",
    "TRAIN_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_gpu_train_{MODEL_NAME}\"\n",
    "SERVE_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_cpu_predict_{MODEL_NAME}\"\n",
    "\n",
    "MACHINE_TYPE = \"n1-standard-8\"\n",
    "REPLICA_COUNT = \"1\"\n",
    "ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
    "ACCELERATOR_COUNT = \"1\"\n",
    "NUM_WORKERS = 1\n",
    "\n",
    "SERVING_HEALTH_ROUTE = \"/ping\"\n",
    "SERVING_PREDICT_ROUTE = f\"/predictions/{MODEL_NAME}\"\n",
    "SERVING_CONTAINER_PORT= [{\"containerPort\": 7080}]\n",
    "SERVING_MACHINE_TYPE = \"n1-standard-4\"\n",
    "SERVING_MIN_REPLICA_COUNT = 1\n",
    "SERVING_MAX_REPLICA_COUNT=1\n",
    "SERVING_TRAFFIC_SPLIT='{\"0\": 100}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cd92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=f\"pytorch-{APP_NAME}\")\n",
    "def pytorch_text_classifier_pipeline(\n",
    "    pipeline_job_id: str,\n",
    "    gs_train_script_path: str,\n",
    "    gs_serving_dependencies_path: str,\n",
    "    eval_acc_threshold: float,\n",
    "    is_hp_tuning_enabled: str = \"n\",\n",
    "):\n",
    "    # ========================================================================\n",
    "    # build custom training container image\n",
    "    # ========================================================================\n",
    "    # build custom container for training job passing the\n",
    "    # GCS location of the training application code\n",
    "    build_custom_train_image_task = (\n",
    "        build_train_image(\n",
    "            project=PROJECT_ID,\n",
    "            gs_train_src_path=gs_train_script_path,\n",
    "            training_image_uri=TRAIN_IMAGE_URI,\n",
    "        )\n",
    "        .set_caching_options(True)\n",
    "        .set_display_name(\"Build custom training image\")\n",
    "    )\n",
    "\n",
    "\n",
    "    # ========================================================================\n",
    "    # model training\n",
    "    # ========================================================================\n",
    "    # train the model on Vertex AI by submitting a CustomJob\n",
    "    # using the custom container (no hyper-parameter tuning)\n",
    "    # define training code arguments\n",
    "    training_args = [\"--num-epochs\", \"2\", \"--model-name\", MODEL_NAME]\n",
    "    # define job name\n",
    "    JOB_NAME = f\"{MODEL_NAME}-train-pytorch-cstm-cntr-{TIMESTAMP}\"\n",
    "    GCS_BASE_OUTPUT_DIR = f\"{GCS_STAGING}/{TIMESTAMP}\"\n",
    "    # define worker pool specs\n",
    "    worker_pool_specs = [\n",
    "        {\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": MACHINE_TYPE,\n",
    "                \"accelerator_type\": ACCELERATOR_TYPE,\n",
    "                \"accelerator_count\": ACCELERATOR_COUNT,\n",
    "            },\n",
    "            \"replica_count\": REPLICA_COUNT,\n",
    "            \"container_spec\": {\"image_uri\": TRAIN_IMAGE_URI, \"args\": training_args},\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    run_train_task = (\n",
    "        custom_job.CustomTrainingJobOp(\n",
    "            project=PROJECT_ID,\n",
    "            location=REGION,\n",
    "            display_name=JOB_NAME,\n",
    "            base_output_directory=GCS_BASE_OUTPUT_DIR,\n",
    "            worker_pool_specs=worker_pool_specs,\n",
    "        )\n",
    "        .set_display_name(\"Run custom training job\")\n",
    "        .after(build_custom_train_image_task)\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # get training job details\n",
    "    # ========================================================================\n",
    "    training_job_details_task = get_training_job_details(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        job_resource=run_train_task.output,\n",
    "        eval_metric_key=\"eval_accuracy\",\n",
    "        model_display_name=MODEL_NAME,\n",
    "    ).set_display_name(\"Get custom training job details\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # model deployment when condition is met\n",
    "    # ========================================================================\n",
    "    with dsl.Condition(\n",
    "        training_job_details_task.outputs[\"eval_metric\"] > eval_acc_threshold,\n",
    "        name=\"model-deploy-decision\",\n",
    "    ):\n",
    "        # ===================================================================\n",
    "        # create model archive file\n",
    "        # ===================================================================\n",
    "        create_mar_task = generate_mar_file(\n",
    "            model_display_name=MODEL_NAME,\n",
    "            model_version=VERSION,\n",
    "            handler=gs_serving_dependencies_path,\n",
    "            model=training_job_details_task.outputs[\"model\"],\n",
    "        ).set_display_name(\"Create MAR file\")\n",
    "\n",
    "        # ===================================================================\n",
    "        # build custom serving container running TorchServe\n",
    "        # ===================================================================\n",
    "        # build custom container for serving predictions using\n",
    "        # the trained model artifacts served by TorchServe\n",
    "        build_custom_serving_image_task = build_custom_serving_image(\n",
    "            project=PROJECT_ID,\n",
    "            gs_serving_dependencies_path=gs_serving_dependencies_path,\n",
    "            serving_image_uri=SERVE_IMAGE_URI,\n",
    "        ).set_display_name(\"Build custom serving image\")\n",
    "\n",
    "        # ===================================================================\n",
    "        # create model resource\n",
    "        # ===================================================================\n",
    "        # upload model to vertex ai\n",
    "        model_upload_task = (\n",
    "            aip_components.ModelUploadOp(\n",
    "                project=PROJECT_ID,\n",
    "                display_name=MODEL_DISPLAY_NAME,\n",
    "                serving_container_image_uri=SERVE_IMAGE_URI,\n",
    "                serving_container_predict_route=SERVING_PREDICT_ROUTE,\n",
    "                serving_container_health_route=SERVING_HEALTH_ROUTE,\n",
    "                serving_container_ports=SERVING_CONTAINER_PORT,\n",
    "                serving_container_environment_variables=create_mar_task.outputs[\n",
    "                    \"mar_env_var\"\n",
    "                ],\n",
    "                artifact_uri=create_mar_task.outputs[\"mar_export_uri\"],\n",
    "            )\n",
    "            .set_display_name(\"Upload model\")\n",
    "            .after(build_custom_serving_image_task)\n",
    "        )\n",
    "\n",
    "        # ===================================================================\n",
    "        # create Vertex AI Endpoint\n",
    "        # ===================================================================\n",
    "        # create endpoint to deploy one or more models\n",
    "        # An endpoint provides a service URL where the prediction requests are sent\n",
    "        endpoint_create_task = (\n",
    "            aip_components.EndpointCreateOp(\n",
    "                project=PROJECT_ID,\n",
    "                display_name=MODEL_NAME + \"-endpoint\",\n",
    "            )\n",
    "            .set_display_name(\"Create endpoint\")\n",
    "            .after(create_mar_task)\n",
    "        )\n",
    "\n",
    "        # ===================================================================\n",
    "        # deploy model to Vertex AI Endpoint\n",
    "        # ===================================================================\n",
    "        # deploy models to endpoint to associates physical resources with the model\n",
    "        # so it can serve online predictions\n",
    "        model_deploy_task = aip_components.ModelDeployOp(\n",
    "            endpoint=endpoint_create_task.outputs[\"endpoint\"],\n",
    "            model=model_upload_task.outputs[\"model\"],\n",
    "            deployed_model_display_name=MODEL_NAME,\n",
    "            dedicated_resources_machine_type=SERVING_MACHINE_TYPE,\n",
    "            dedicated_resources_min_replica_count=SERVING_MIN_REPLICA_COUNT,\n",
    "            dedicated_resources_max_replica_count=SERVING_MAX_REPLICA_COUNT,\n",
    "            traffic_split=SERVING_TRAFFIC_SPLIT,\n",
    "        ).set_display_name(\"Deploy model to endpoint\")\n",
    "\n",
    "        # ===================================================================\n",
    "        # test model deployment\n",
    "        # ===================================================================\n",
    "        # test model deployment by making online prediction requests\n",
    "        test_instances = [\n",
    "            \"Jaw dropping visual affects and action! One of the best I have seen to date.\",\n",
    "            \"Take away the CGI and the A-list cast and you end up with film with less punch.\",\n",
    "        ]\n",
    "        predict_test_instances_task = make_prediction_request(\n",
    "            project=PROJECT_ID,\n",
    "            bucket=BUCKET,\n",
    "            endpoint=model_deploy_task.outputs[\"gcp_resources\"],\n",
    "            instances=test_instances,\n",
    "        ).set_display_name(\"Test model deployment making online predictions\")\n",
    "        predict_test_instances_task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3455128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_JSON_SPEC_PATH = \"./tmp/pipelines/pytorch_text_classifier_pipeline_spec.json\"\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pytorch_text_classifier_pipeline,\n",
    "    package_path=PIPELINE_JSON_SPEC_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f935e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=PROJECT_REGION)\n",
    "\n",
    "# define pipeline parameters\n",
    "# NOTE: These parameters can be included in the pipeline config file as needed\n",
    "PIPELINE_JOB_ID = f\"pipeline-{APP_NAME}-{get_timestamp()}\"\n",
    "TRAIN_APP_CODE_PATH = f\"{BUCKET}/{APP_NAME}/train/\"\n",
    "SERVE_DEPENDENCIES_PATH = f\"{BUCKET}/{APP_NAME}/serve/\"\n",
    "print(PIPELINE_JOB_ID)\n",
    "print(TRAIN_APP_CODE_PATH)\n",
    "\n",
    "print(SERVE_DEPENDENCIES_PATH)\n",
    "\n",
    "pipeline_params = {\n",
    "    \"pipeline_job_id\": PIPELINE_JOB_ID,\n",
    "    \"gs_train_script_path\": TRAIN_APP_CODE_PATH,\n",
    "    \"gs_serving_dependencies_path\": SERVE_DEPENDENCIES_PATH,\n",
    "    \"eval_acc_threshold\": 0.87,\n",
    "    \"is_hp_tuning_enabled\": \"n\",\n",
    "}\n",
    "\n",
    "\n",
    "# define pipeline job\n",
    "pipeline_job = pipeline_jobs.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    job_id=PIPELINE_JOB_ID,\n",
    "    template_path=PIPELINE_JSON_SPEC_PATH,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values=pipeline_params,\n",
    "    enable_caching=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03b859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit pipeline job for execution\n",
    "response = pipeline_job.run(sync=True)\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-research-XyzUro3q-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "4d64233ef496e27adda18c9013bba942dfebf360eb2fd79c82dd8d4422c2662f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
